\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{mcculloch1943logical}
\citation{krizhevsky2012imagenet}
\citation{girshick2015fast}
\citation{graves2013hybrid}
\citation{lecun1990handwritten}
\citation{vincent2010stacked}
\citation{chapelle2009semi}
\citation{le2013building}
\citation{socher2011semi}
\Newlabel{a}{a}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{main}{{1}{1}{Introduction}{section.1}{}}
\citation{chapelle2003cluster}
\citation{chapelle2005semi}
\citation{zhu2002learning}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work and backgrounds}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Semi-supervised learning for neural network}{2}{subsection.2.1}}
\citation{zhu2002learning}
\citation{weston2012deep}
\newlabel{eq:1}{{1}{3}{Semi-supervised learning for neural network}{equation.2.1}{}}
\newlabel{eq:2}{{2}{3}{Semi-supervised learning for neural network}{equation.2.2}{}}
\newlabel{eq:3}{{3}{3}{Semi-supervised learning for neural network}{equation.2.3}{}}
\newlabel{eq:4}{{4}{3}{Semi-supervised learning for neural network}{equation.2.4}{}}
\newlabel{eq:5}{{5}{3}{Semi-supervised learning for neural network}{equation.2.5}{}}
\newlabel{eq:6}{{6}{3}{Semi-supervised learning for neural network}{equation.2.6}{}}
\citation{kohonen1998self}
\citation{srivastava2014dropout}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Traditional way for semi-supervised embedding}{4}{subsection.2.2}}
\newlabel{eq:7}{{7}{4}{Traditional way for semi-supervised embedding}{equation.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Self Organizing Map}{4}{subsection.2.3}}
\newlabel{eq:8}{{8}{4}{Self Organizing Map}{equation.2.8}{}}
\newlabel{eq:9}{{9}{4}{Self Organizing Map}{equation.2.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Our architecture: the Hybrid Learning Net}{4}{section.3}}
\newlabel{eq:10}{{10}{4}{Our architecture: the Hybrid Learning Net}{equation.3.10}{}}
\newlabel{eq:11}{{11}{4}{Our architecture: the Hybrid Learning Net}{equation.3.11}{}}
\newlabel{eq:12}{{12}{4}{Our architecture: the Hybrid Learning Net}{equation.3.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The left is FCN architecture, the right is HLN architecture. The big solid circles are the neurons, the medium ones are the middle states of neurons, and the small ones are the vector maps $\mathbf  {m}^k$ of SOMs, finally the tiny ones are the function modules. The parts omitted refers to repeating hidden layers connected in same way.}}{5}{figure.1}}
\newlabel{fig:1}{{1}{5}{The left is FCN architecture, the right is HLN architecture. The big solid circles are the neurons, the medium ones are the middle states of neurons, and the small ones are the vector maps $\mathbf {m}^k$ of SOMs, finally the tiny ones are the function modules. The parts omitted refers to repeating hidden layers connected in same way}{figure.1}{}}
\newlabel{eq:13}{{13}{5}{Our architecture: the Hybrid Learning Net}{equation.3.13}{}}
\newlabel{eq:14}{{14}{5}{Our architecture: the Hybrid Learning Net}{equation.3.14}{}}
\newlabel{eq:15}{{15}{5}{Our architecture: the Hybrid Learning Net}{equation.3.15}{}}
\newlabel{eq:16}{{16}{6}{Our architecture: the Hybrid Learning Net}{equation.3.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}The training theory for HLNs}{6}{section.4}}
\newlabel{eq:17}{{17}{6}{The training theory for HLNs}{equation.4.17}{}}
\newlabel{eq:18}{{18}{6}{The training theory for HLNs}{equation.4.18}{}}
\newlabel{eq:19}{{19}{6}{The training theory for HLNs}{equation.4.19}{}}
\newlabel{eq:20}{{20}{6}{The training theory for HLNs}{equation.4.20}{}}
\newlabel{eq:21}{{21}{6}{The training theory for HLNs}{equation.4.21}{}}
\newlabel{eq:22}{{22}{6}{The training theory for HLNs}{equation.4.22}{}}
\newlabel{eq:23}{{23}{7}{The training theory for HLNs}{equation.4.23}{}}
\newlabel{eq:24}{{24}{7}{The training theory for HLNs}{equation.4.24}{}}
\newlabel{eq:25}{{25}{7}{The training theory for HLNs}{equation.4.25}{}}
\newlabel{eq:26}{{26}{7}{The training theory for HLNs}{equation.4.26}{}}
\newlabel{eq:27}{{27}{7}{The training theory for HLNs}{equation.4.27}{}}
\newlabel{eq:28}{{28}{7}{The training theory for HLNs}{equation.4.28}{}}
\newlabel{eq:29}{{29}{7}{The training theory for HLNs}{equation.4.29}{}}
\newlabel{eq:30}{{30}{7}{The training theory for HLNs}{equation.4.30}{}}
\newlabel{eq:31}{{31}{7}{The training theory for HLNs}{equation.4.31}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Empirical Study}{7}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Regression capability experiment using small synthetic data}{7}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Comparison on mean of accuracy with synthetic data: $\mu $ is the mean accuracy for multiple replays with the same configuration. $N_{pre}$ is the unlabeled sample volume for pretraining, $\eta $ is the initial learning rate, $\alpha $ is the decay factor of learning rate $\eta (t)$. }}{8}{figure.2}}
\newlabel{fig:2}{{2}{8}{Comparison on mean of accuracy with synthetic data: $\mu $ is the mean accuracy for multiple replays with the same configuration. $N_{pre}$ is the unlabeled sample volume for pretraining, $\eta $ is the initial learning rate, $\alpha $ is the decay factor of learning rate $\eta (t)$}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Comparison on deviation of accuracy with synthetic data: the deviation takes the form as $\delta $ is the deviation of multiple replays with the same configuration as $\delta =\DOTSB \sum@ \slimits@ _1^N(\gamma _i-\mu )/N$ , where $\gamma _i$ is $i^{th}$ replay accuracy. }}{8}{figure.3}}
\newlabel{fig:3}{{3}{8}{Comparison on deviation of accuracy with synthetic data: the deviation takes the form as $\delta $ is the deviation of multiple replays with the same configuration as $\delta =\sum _1^N(\gamma _i-\mu )/N$ , where $\gamma _i$ is $i^{th}$ replay accuracy}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Experiments on MNIST}{8}{subsection.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Comparison of testing accuracy on MNIST with 4 different network model: $196I$ refers to the input layer of dimension 196, $30H$ refers to the hidden layer of dimension of 30, and $10F$ is the fully connected output layer of dimension 10. }}{9}{figure.4}}
\newlabel{fig:4}{{4}{9}{Comparison of testing accuracy on MNIST with 4 different network model: $196I$ refers to the input layer of dimension 196, $30H$ refers to the hidden layer of dimension of 30, and $10F$ is the fully connected output layer of dimension 10}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Comparison of training loss on MNIST with 4 different network models }}{9}{figure.5}}
\newlabel{fig:5}{{5}{9}{Comparison of training loss on MNIST with 4 different network models}{figure.5}{}}
\bibstyle{elsarticle-num}
\bibdata{ref}
\bibcite{mcculloch1943logical}{{1}{}{{}}{{}}}
\bibcite{krizhevsky2012imagenet}{{2}{}{{}}{{}}}
\bibcite{girshick2015fast}{{3}{}{{}}{{}}}
\bibcite{graves2013hybrid}{{4}{}{{}}{{}}}
\bibcite{lecun1990handwritten}{{5}{}{{}}{{}}}
\bibcite{vincent2010stacked}{{6}{}{{}}{{}}}
\bibcite{chapelle2009semi}{{7}{}{{}}{{}}}
\bibcite{le2013building}{{8}{}{{}}{{}}}
\bibcite{socher2011semi}{{9}{}{{}}{{}}}
\bibcite{chapelle2003cluster}{{10}{}{{}}{{}}}
\bibcite{chapelle2005semi}{{11}{}{{}}{{}}}
\bibcite{zhu2002learning}{{12}{}{{}}{{}}}
\bibcite{weston2012deep}{{13}{}{{}}{{}}}
\bibcite{kohonen1998self}{{14}{}{{}}{{}}}
\bibcite{srivastava2014dropout}{{15}{}{{}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Comparison of neuron activation sparsity on MNIST with 4 different network models }}{10}{figure.6}}
\newlabel{fig:6}{{6}{10}{Comparison of neuron activation sparsity on MNIST with 4 different network models}{figure.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions}{10}{section.6}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\let\numberline\tmptocnumberline}
\@writefile{toc}{\contentsline {section}{\numberline {Appendix \nobreakspace  {}A}An example appendix}{11}{appendix.A}}
\@writefile{toc}{\contentsline {subsection}{\numberline {Appendix \nobreakspace  {}A.1}Example of a sub-heading within an appendix}{11}{subsection.A.1}}
\newlabel{lastpage}{{Appendix \nobreakspace  {}A.1}{11}{Example of a sub-heading within an appendix}{subsection.A.1}{}}
