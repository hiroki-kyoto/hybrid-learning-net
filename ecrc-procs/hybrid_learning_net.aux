\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{mcculloch1943logical}
\citation{krizhevsky2012imagenet}
\citation{girshick2015fast}
\citation{graves2013hybrid}
\citation{bottou2010large}
\citation{lecun1990handwritten}
\citation{vincent2010stacked}
\citation{chapelle2009semi}
\citation{le2013building}
\citation{socher2011semi}
\Newlabel{u1}{a}
\Newlabel{u2}{b}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{main}{{1}{1}{Introduction}{section.1}{}}
\citation{chapelle2003cluster}
\citation{chapelle2005semi}
\citation{zhu2002learning}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work and background Knowledge}{2}{section.2}}
\newlabel{sec:related_work}{{2}{2}{Related work and background Knowledge}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Semi-supervised learning for neural network}{2}{subsection.2.1}}
\newlabel{eq:1}{{1}{2}{Semi-supervised learning for neural network}{equation.2.1}{}}
\citation{kohonen1998self}
\citation{srivastava2014dropout}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Self Organizing Map}{3}{subsection.2.2}}
\newlabel{eq:8}{{3}{3}{Self Organizing Map}{equation.2.3}{}}
\newlabel{eq:9}{{4}{3}{Self Organizing Map}{equation.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Hybrid Learning Network}{3}{section.3}}
\newlabel{eq:10}{{5}{3}{Hybrid Learning Network}{equation.3.5}{}}
\newlabel{eq:11}{{6}{3}{Hybrid Learning Network}{equation.3.6}{}}
\newlabel{eq:12}{{7}{3}{Hybrid Learning Network}{equation.3.7}{}}
\newlabel{fig:1:a}{{1(a)}{4}{Subfigure 1(a)}{subfigure.1.1}{}}
\newlabel{sub@fig:1:a}{{(a)}{4}{Subfigure 1(a)\relax }{subfigure.1.1}{}}
\newlabel{fig:1:b}{{1(b)}{4}{Subfigure 1(b)}{subfigure.1.2}{}}
\newlabel{sub@fig:1:b}{{(b)}{4}{Subfigure 1(b)\relax }{subfigure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The left is FCN architecture, the right is HLN architecture. The big solid circles are the neurons, the medium ones are the middle states of neurons, and the small ones are the vector maps $\mathbf  {m}^k$ of SOMs, finally the tiny ones are the function modules. The parts omitted refers to repeating hidden layers connected in same way.}}{4}{figure.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {The FCN architecture}}}{4}{figure.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {The HLN architecture}}}{4}{figure.1}}
\newlabel{fig:1}{{1}{4}{The left is FCN architecture, the right is HLN architecture. The big solid circles are the neurons, the medium ones are the middle states of neurons, and the small ones are the vector maps $\mathbf {m}^k$ of SOMs, finally the tiny ones are the function modules. The parts omitted refers to repeating hidden layers connected in same way}{figure.1}{}}
\newlabel{eq:13}{{8}{4}{Hybrid Learning Network}{equation.3.8}{}}
\newlabel{eq:14}{{9}{4}{Hybrid Learning Network}{equation.3.9}{}}
\newlabel{eq:15}{{10}{4}{Hybrid Learning Network}{equation.3.10}{}}
\newlabel{eq:16}{{11}{4}{Hybrid Learning Network}{equation.3.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}HLN Training}{5}{section.4}}
\newlabel{eq:17}{{12}{5}{HLN Training}{equation.4.12}{}}
\newlabel{eq:18}{{13}{5}{HLN Training}{equation.4.13}{}}
\newlabel{eq:19}{{14}{5}{HLN Training}{equation.4.14}{}}
\newlabel{eq:20}{{15}{5}{HLN Training}{equation.4.15}{}}
\newlabel{eq:23}{{16}{5}{HLN Training}{equation.4.16}{}}
\newlabel{eq:24}{{17}{5}{HLN Training}{equation.4.17}{}}
\newlabel{eq:25}{{18}{5}{HLN Training}{equation.4.18}{}}
\newlabel{eq:27}{{19}{5}{HLN Training}{equation.4.19}{}}
\newlabel{eq:29}{{20}{6}{HLN Training}{equation.4.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Empirical Study}{6}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Synthetic data}{6}{subsection.5.1}}
\newlabel{fig:2:a}{{2(a)}{6}{Subfigure 2(a)}{subfigure.2.1}{}}
\newlabel{sub@fig:2:a}{{(a)}{6}{Subfigure 2(a)\relax }{subfigure.2.1}{}}
\newlabel{fig:2:b}{{2(b)}{6}{Subfigure 2(b)}{subfigure.2.2}{}}
\newlabel{sub@fig:2:b}{{(b)}{6}{Subfigure 2(b)\relax }{subfigure.2.2}{}}
\newlabel{fig:2:c}{{2(c)}{6}{Subfigure 2(c)}{subfigure.2.3}{}}
\newlabel{sub@fig:2:c}{{(c)}{6}{Subfigure 2(c)\relax }{subfigure.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Mean of accuracy on the synthetic data: $\mu $ is the mean accuracy for multiple replays with the same configuration. $N_{pre}$ is the number of unlabeled samples for pretraining, $\eta $ is the initial learning rate, $\alpha $ is the decay factor of learning rate $\eta (t)$. }}{6}{figure.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{6}{figure.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{6}{figure.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{6}{figure.2}}
\newlabel{fig:2}{{2}{6}{Mean of accuracy on the synthetic data: $\mu $ is the mean accuracy for multiple replays with the same configuration. $N_{pre}$ is the number of unlabeled samples for pretraining, $\eta $ is the initial learning rate, $\alpha $ is the decay factor of learning rate $\eta (t)$}{figure.2}{}}
\newlabel{fig:3:a}{{3(a)}{6}{Subfigure 3(a)}{subfigure.3.1}{}}
\newlabel{sub@fig:3:a}{{(a)}{6}{Subfigure 3(a)\relax }{subfigure.3.1}{}}
\newlabel{fig:3:b}{{3(b)}{6}{Subfigure 3(b)}{subfigure.3.2}{}}
\newlabel{sub@fig:3:b}{{(b)}{6}{Subfigure 3(b)\relax }{subfigure.3.2}{}}
\newlabel{fig:3:c}{{3(c)}{6}{Subfigure 3(c)}{subfigure.3.3}{}}
\newlabel{sub@fig:3:c}{{(c)}{6}{Subfigure 3(c)\relax }{subfigure.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Deviation of accuracy on the synthetic data: the deviation takes the form as $\delta $ is the deviation of multiple replays with the same configuration $\delta =\DOTSB \sum@ \slimits@ _1^N(\gamma _i-\mu )/N$ , where $\gamma _i$ is the accuracy of $i^{th}$ replay. }}{6}{figure.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{6}{figure.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{6}{figure.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{6}{figure.3}}
\newlabel{fig:3}{{3}{6}{Deviation of accuracy on the synthetic data: the deviation takes the form as $\delta $ is the deviation of multiple replays with the same configuration $\delta =\sum _1^N(\gamma _i-\mu )/N$ , where $\gamma _i$ is the accuracy of $i^{th}$ replay}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}MNIST}{7}{subsection.5.2}}
\newlabel{fig:4:a}{{4(a)}{7}{Subfigure 4(a)}{subfigure.4.1}{}}
\newlabel{sub@fig:4:a}{{(a)}{7}{Subfigure 4(a)\relax }{subfigure.4.1}{}}
\newlabel{fig:4:b}{{4(b)}{7}{Subfigure 4(b)}{subfigure.4.2}{}}
\newlabel{sub@fig:4:b}{{(b)}{7}{Subfigure 4(b)\relax }{subfigure.4.2}{}}
\newlabel{fig:4:c}{{4(c)}{7}{Subfigure 4(c)}{subfigure.4.3}{}}
\newlabel{sub@fig:4:c}{{(c)}{7}{Subfigure 4(c)\relax }{subfigure.4.3}{}}
\newlabel{fig:4:d}{{4(d)}{7}{Subfigure 4(d)}{subfigure.4.4}{}}
\newlabel{sub@fig:4:d}{{(d)}{7}{Subfigure 4(d)\relax }{subfigure.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Accuracy on MNIST with 4 different network models: $196I$ denotes the input layer of dimension 196, $30H$ denotes the hidden layer of dimension of 30, and $10F$ denotes the fully connected output layer of dimension 10. }}{7}{figure.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{7}{figure.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{7}{figure.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{7}{figure.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{7}{figure.4}}
\newlabel{fig:4}{{4}{7}{Accuracy on MNIST with 4 different network models: $196I$ denotes the input layer of dimension 196, $30H$ denotes the hidden layer of dimension of 30, and $10F$ denotes the fully connected output layer of dimension 10}{figure.4}{}}
\newlabel{fig:5:a}{{5(a)}{8}{Subfigure 5(a)}{subfigure.5.1}{}}
\newlabel{sub@fig:5:a}{{(a)}{8}{Subfigure 5(a)\relax }{subfigure.5.1}{}}
\newlabel{fig:5:b}{{5(b)}{8}{Subfigure 5(b)}{subfigure.5.2}{}}
\newlabel{sub@fig:5:b}{{(b)}{8}{Subfigure 5(b)\relax }{subfigure.5.2}{}}
\newlabel{fig:5:c}{{5(c)}{8}{Subfigure 5(c)}{subfigure.5.3}{}}
\newlabel{sub@fig:5:c}{{(c)}{8}{Subfigure 5(c)\relax }{subfigure.5.3}{}}
\newlabel{fig:5:d}{{5(d)}{8}{Subfigure 5(d)}{subfigure.5.4}{}}
\newlabel{sub@fig:5:d}{{(d)}{8}{Subfigure 5(d)\relax }{subfigure.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Training loss on MNIST with 4 different network models }}{8}{figure.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{8}{figure.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{8}{figure.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{8}{figure.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{8}{figure.5}}
\newlabel{fig:5}{{5}{8}{Training loss on MNIST with 4 different network models}{figure.5}{}}
\bibstyle{elsarticle-num}
\bibdata{ref}
\bibcite{mcculloch1943logical}{{1}{}{{}}{{}}}
\newlabel{fig:6:a}{{6(a)}{9}{Subfigure 6(a)}{subfigure.6.1}{}}
\newlabel{sub@fig:6:a}{{(a)}{9}{Subfigure 6(a)\relax }{subfigure.6.1}{}}
\newlabel{fig:6:b}{{6(b)}{9}{Subfigure 6(b)}{subfigure.6.2}{}}
\newlabel{sub@fig:6:b}{{(b)}{9}{Subfigure 6(b)\relax }{subfigure.6.2}{}}
\newlabel{fig:6:c}{{6(c)}{9}{Subfigure 6(c)}{subfigure.6.3}{}}
\newlabel{sub@fig:6:c}{{(c)}{9}{Subfigure 6(c)\relax }{subfigure.6.3}{}}
\newlabel{fig:6:d}{{6(d)}{9}{Subfigure 6(d)}{subfigure.6.4}{}}
\newlabel{sub@fig:6:d}{{(d)}{9}{Subfigure 6(d)\relax }{subfigure.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Neuron activation sparsity on MNIST with 4 different network models: H0 denotes the first hidden layer, H1 denotes the second hideen layer. }}{9}{figure.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{9}{figure.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{9}{figure.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{9}{figure.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{9}{figure.6}}
\newlabel{fig:6}{{6}{9}{Neuron activation sparsity on MNIST with 4 different network models: H0 denotes the first hidden layer, H1 denotes the second hideen layer}{figure.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions}{9}{section.6}}
\bibcite{krizhevsky2012imagenet}{{2}{}{{}}{{}}}
\bibcite{girshick2015fast}{{3}{}{{}}{{}}}
\bibcite{graves2013hybrid}{{4}{}{{}}{{}}}
\bibcite{bottou2010large}{{5}{}{{}}{{}}}
\bibcite{lecun1990handwritten}{{6}{}{{}}{{}}}
\bibcite{vincent2010stacked}{{7}{}{{}}{{}}}
\bibcite{chapelle2009semi}{{8}{}{{}}{{}}}
\bibcite{le2013building}{{9}{}{{}}{{}}}
\bibcite{socher2011semi}{{10}{}{{}}{{}}}
\bibcite{chapelle2003cluster}{{11}{}{{}}{{}}}
\bibcite{chapelle2005semi}{{12}{}{{}}{{}}}
\bibcite{zhu2002learning}{{13}{}{{}}{{}}}
\bibcite{kohonen1998self}{{14}{}{{}}{{}}}
\bibcite{srivastava2014dropout}{{15}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\newlabel{lastpage}{{6}{10}{Acknowledgements}{section*.2}{}}
