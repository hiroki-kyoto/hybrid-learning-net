
% Template for Elsevier CRC journal article
% version 1.2 dated 09 May 2011

% This file (c) 2009-2011 Elsevier Ltd.  Modifications may be freely made,
% provided the edited file is saved under a different name

% This file contains modifications for Procedia Computer Science

% Changes since version 1.1
% - added "procedia" option compliant with ecrc.sty version 1.2a
%   (makes the layout approximately the same as the Word CRC template)
% - added example for generating copyright line in abstract

%-----------------------------------------------------------------------------------

%% This template uses the elsarticle.cls document class and the extension package ecrc.sty
%% For full documentation on usage of elsarticle.cls, consult the documentation "elsdoc.pdf"
%% Further resources available at http://www.elsevier.com/latex

%-----------------------------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                          %%
%% Important note on usage                                  %%
%% -----------------------                                  %%
%% This file should normally be compiled with PDFLaTeX      %%
%% Using standard LaTeX should work but may produce clashes %%
%%                                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% The '3p' and 'times' class options of elsarticle are used for Elsevier CRC
%% The 'procedia' option causes ecrc to approximate to the Word template
\documentclass[3p,times,procedia]{elsarticle}

%% The `ecrc' package must be called to make the CRC functionality available
\usepackage{ecrc}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{subfigure}

%% The ecrc package defines commands needed for running heads and logos.
%% For running heads, you can set the journal name, the volume, the starting page and the authors

%% set the volume if you know. Otherwise `00'
\volume{00}

%% set the starting page if not 1
\firstpage{1}

%% Give the name of the journal
\journalname{Procedia Computer Science}

%% Give the author list to appear in the running head
%% Example \runauth{C.V. Radhakrishnan et al.}
\runauth{Ying Liu, Chao Xiang}

%% The choice of journal logo is determined by the \jid and \jnltitlelogo commands.
%% A user-supplied logo with the name <\jid>logo.pdf will be inserted if present.
%% e.g. if \jid{yspmi} the system will look for a file yspmilogo.pdf
%% Otherwise the content of \jnltitlelogo will be set between horizontal lines as a default logo

%% Give the abbreviation of the Journal.
\jid{procs}

%% Give a short journal name for the dummy logo (if needed)
\jnltitlelogo{Procedia Computer Science}

%% Provide the copyright line to appear in the abstract
%% Usage:
%   \CopyrightLine[<text-before-year>]{<year>}{<restt-of-the-copyright-text>}
%   \CopyrightLine[Crown copyright]{2011}{Published by Elsevier Ltd.}
%   \CopyrightLine{2011}{Elsevier Ltd. All rights reserved}
\CopyrightLine{2017}{The Authors. Published by Elsevier B.V.\newline Selection and/or peer-review under responsibility of ITQM2017}


%% Hereafter the template follows `elsarticle'.
%% For more details see the existing template files elsarticle-template-harv.tex and elsarticle-template-num.tex.

%% Elsevier CRC generally uses a numbered reference style
%% For this, the conventions of elsarticle-template-num.tex should be followed (included below)
%% If using BibTeX, use the style file elsarticle-num.bst

%% End of ecrc-specific commands
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}

% if you have landscape tables
\usepackage[figuresright]{rotating}

% put your own definitions here:
%   \newcommand{\cZ}{\cal{Z}}
%   \newtheorem{def}{Definition}[section]
%   ...

% add words to TeX's hyphenation exception list
%\hyphenation{author another created financial paper re-commend-ed Post-Script}

% declarations for front matter

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\dochead{Information Technology and Quantitative Management (ITQM 2017)}
%% Use \dochead if there is an article header, e.g. \dochead{Short communication}
%% \dochead can also be used to include a conference title, if directed by the editors
%% e.g. \dochead{17th International Conference on Dynamical Processes in Excited States of Solids}

\title{Hybrid Learning Network: 
	A Novel Architecture for
	Fast Learning}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author[u1,u2]{Ying Liu}
\author[u1]{Chao Xiang}

\address[u1]{School of Computer 
	and Engineering, University of 
	Chinese Academy of Sciences, 
	Beijing, 100190 China}
\address[u2]{Key Lab of Big Data
	Mining and Knowledge Management,
	Chinese Academy of Sciences,
	Beijing, 100190 China}

\begin{abstract}
%% Text of abstract
There're many efficient architectures 
of the artificial neural network(ANN).
For which the training is a hard work.
The cost for training an ANN increases
exponentially when the ANN gets deeper 
or wider. 
We therefore propose a novel 
achitecture, the Hybrid Learning
Network(HLN), to achieve a fast learning 
with good stablity.
The HLN can learn from both
labeled data and unlabeled data
at the same time in a hybrid
learning manner.
It uses a Self Organizing Map unified
by the specially designed nonlinear
function as the sparsity mask 
for a hidden layer to improve 
the training speed.
We experiment our architecture on 
a synthetic dataset to test its 
regression capability against the
traditional architecture, the result 
is promising.
We also experiment on the well-known
MNIST dataset, it demonstrates an 
even more impressive learning efficiency 
by up to $40\times$ speed-up on training.
\end{abstract}

\begin{keyword}
HLN; hybrid learning; neural network; sparsity mask; SOM; MNIST; fast learning

%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

\correspondingauthor[*]{Corresponding author. Tel.: +86-183-0114-2368;}
\email{xiangchao215@mails.ucas.ac.cn}

%%
%% Start line numbering here if you want
%%
% \linenumbers

%% main text
\section{Introduction}
\label{main}
Since the first mathematical model 
of the artificial neural network(ANN) was 
proposed in 
1943\cite{mcculloch1943logical}, 
lots of different architectures 
have been proposed
to develop the model, such as 
the Convolutional Neural Network(CNN) 
for image recognition
\cite{krizhevsky2012imagenet}, 
the Recurrent Convolutional Neural 
Network(R-CNN) for object 
detection in videos
\cite{girshick2015fast}, 
and the Long Short Term Memory(LSTM)
for speech recognition
\cite{graves2013hybrid}, etc. 
These specially designed neural 
networks are trained by 
a few efficient methods such as
the stochastic gradient 
descent(SGD)\cite{bottou2010large}. 
Such architectures
suit well for their specific 
applications but may have plain 
or worse performances on others, 
and their best performances 
rely heavily on the hyperparameter 
configuration. Therefore
a relatively universal 
architecutre that enables equal 
or similar performances among 
varied applications is in real demand.

Although a number of
different neural network architectures
have been proposed in the past years,
literally all of them can be classifed
into 3 categories, supervised
\cite{lecun1990handwritten}, 
unsupervised
\cite{vincent2010stacked}, 
and semi-supervised learning
\cite{chapelle2009semi}.
In supervised learning, 
one can only train a model on
labeled samples. However in real 
applications, labeling a large 
dataset is infeasible as a result
unlabeled samples are 
the major. To make use of 
unlabeled data, a few 
unsupervised training algorithms 
emerged. These unsupervised learning
methods attempt to produce an optimized 
parameter initialization
\cite{le2013building}.
Thus, such unsupervised techniques 
can only be applied before the 
supervised training phase. Once 
the supervised begins, 
unsupversied learning will quit.
Semi-supervised learning
allows the model to learn from
both labeled and unlabeled samples
at the same time, where a
regularizer is embedded in
the supervised optimizing object,
and generally a balance constraint is
required to avoid the trival solution
\cite{socher2011semi}.
Such a enhanced learner brings more 
hyperparameters(such as the balance 
constraint), making it even more 
difficult to search for the best 
settings for current architectures. 
Additionally, this integration manner 
makes it impossible to separate the 
two learners as the semi-supervised 
regularizer is built on the supervised 
learner, and therefore an 
early supervised training alone with 
profound labeled data is necessary
before the semi-supervised learning 
starts.

Therefore in the paper, we introduce 
a new architecture that combines the
supervised and unsupervised learning
equiring no extra techniques in
training. Our architecture, the Hybrid
Learning Network(HLN), consists of 
stacked hybrid learning layers, 
each of which
embeds a Self Organizing
Map(SOM). In the similar manner of 
backpropagation, HLNs
demonstrate higher learning
capability and stablity. 

%\subsection{Our contributes}
The main contributions of our work are:
\begin{itemize}
\item
	HLN is proposed as a hybrid learning
	architecture to learn both unlabeled 
		and labeled data simultaneously.
		HLN overcomes
the problem of traditional 
semi-supervised learning methods which 
requires
an early standalone training for the
supervised learner.
\item A SOM-embedding layer structure is 
	designed to learn a cluster 
mapping function from unlabeled data
		to speed up the supervised
		learning from labeled
data.
\item A nonlinear function $h(x)$,
	is proposed to 
		measure the state of a
		SOM in each iteration of the 
		training, and then determine 
		a sparsity
		mask for every hidden layer.
\end{itemize}
Our proposed HLN is implemented in 
Python and all
our code and results of experiments
are available at
\url{https://github.com/hiroki-kyoto/hybrid-learning-net}.

%\subsection{The organization of this artical}
The rest of the paper is organized
as follows.
In section 2 we introduce existing
semi-supervised algorithms for neural
network models and SOM.
In section 3, we present our novel
architecture HLN and how to
embed SOMs into deep architectures of
neural networks.
In section 4 we explain the 
training theory for HLNs.
Section 5 presents the experimental
results, and the
last section concludes our work.

\section{Related work and background
Knowledge}

\subsection{Semi-supervised learning for
neural network}
A key assumption in semi-supervised 
algorithms, is the 
structure assumption: two samples with 
similar distribution on the same mapping 
structure tend to have high probability 
belonging to the same class. Based on 
this assumption, one can use large 
unlabeled data to uncover such 
structures. There're already a few 
algorithms dedicated to it,
such as cluster kernels
\cite{chapelle2003cluster},
Low Density Separation(LDS)
\cite{chapelle2005semi},
label propagation
\cite{zhu2002learning},
etc.
In such algorithms, designing 
a regularizer
to enable the model to learn 
the representation
or structure of raw data
becomes the key point.

Let's firstly focus on the 
general algorithm
description of semi-supervised learning.
Given a set of unlabeled samples,
$\mathbf{X}=\{\mathbf{x}_1,\cdots,
\mathbf{x}_N\}(\mathbf{x}\in\mathbb{R}^d)$,
and the similarity labels between any
$\mathbf{x}_i$ and $\mathbf{x}_j$,
$\mathbf{W}=\{W_{ij}|i,j=1,\cdots,N\}$,
our goal is to find the best 
embedding function,$f(\mathbf{x})$, 
for each sample $\mathbf{x}_i$, 
in order to minimize:
\begin{equation}
\Delta_{f}=
	\sum^{N}_{i=1}
	\sum^{N}_{j=1}
	L\left(
	f(\mathbf{x}_i),
	f(\mathbf{x}_j),
	W_{ij}
	\right)
\label{eq:1}
\end{equation}
where,
\begin{itemize}[]
\item $L(\cdot)$ is the loss 
function of 3 
variables: 
$\left<
		f(\mathbf{x}_i),
		f(\mathbf{x}_j),
		W_{ij}
		\right>$.
For example, if Euclidean distance
is used,
		$$L\left(
		f(\mathbf{x}_i),
		f(\mathbf{x}_j),
		W_{ij}
		\right) = 
		\left(
		\|f(\mathbf{x}_i)-
		f(\mathbf{x}_j)
		\|-W_{ij}
		\right)^2$$
\item $f(\mathbf{x})\in\mathbb{R}^n$ 
is the 
embedding function, 
trying to produce an output
vector for $\mathbf{x}_i$, 
similar to that 
for $\mathbf{x}_j$ with $W_{ij}=0$,
and disimilar with $W_{ij}=1$.
\item $W_{ij}\in \mathbb{R}$ is 
the similarity 
label of the sample pair
$\left<
		\mathbf{x}_i,
		\mathbf{x}_j
		\right>$ from $\mathbf{X}$.
\end{itemize}
As a result, the overall optimization 
object is
\begin{equation}
	\arg \min 
	\sum_i
	\ell\left(
	f(\mathbf{x}_i),
	\mathbf{\hat{y}}_i
	\right)+
	\lambda\Delta_f
\end{equation}
where $\ell(\cdot)$ is the loss
function for supervised learning.
$\lambda$ is proposed 
as the balance 
coefficient to embed
the semi-supervised 
regularizer $\Delta_f$.

\subsection{Self Organizing Map}
Self Organizing Map is an effective 
method for the visualization of 
high-dimensional data. 
It can also be used 
as an automatic clustering method. 
The SOM consists of a two-dimensional 
regular grid of nodes. 
The models are automatically 
organized into a meaningful 
two-dimensional order in which 
similar models are closer to
each other in the grid than 
the more dissimilar
ones\cite{kohonen1998self}.
Rules used to update models are:
\begin{equation}
	\mathbf{m}_i(t+1)=\mathbf{m}_i(t)+
	h_{c(x),i}\left(
	\mathbf{x}(t)-\mathbf{m}_i(t)
	\right)
	\label{eq:8}
\end{equation}
and the learning rate is 
dynamically determined as
\begin{equation}
	h_{c(x),i} = \alpha(t)\exp\left(
	-\frac{\|\mathbf{r}_i-\mathbf{r}_c\|^2}
	{2\sigma^2(t)}
	\right)
	\label{eq:9}
\end{equation}
where $\mathbf{m}_i\in\mathbb{R}^n$ 
denotes the $i^{th}$ model vector, 
$\mathbf{x}$ is an input pattern,
$c(x)$ relates to the best match 
vector index
in $\mathbf{m}$ for input 
pattern $\mathbf{x}$,
and $\alpha(t)$ is a learning rate 
that decreases as the
training preceeds.
$\mathbf{r}$ denotes the model vector 
location in 
the map, and $\sigma(t)$ 
corresponds to the 
width of the neighborhood 
function, which 
also decreases monotonically 
with the regression steps.

\section{Hybrid Learning Network}
Hybrid Learning Network is a novel
architecture aiming at 
enhancing arbitrary networks
on their training efficiency
stablity.
Each hidden layer in HLN architecture 
is embedded with a SOM. For the 
simplest case where neurons are connected 
with fully connection, the traditional
architecture, called Fully 
Connected Network(FCN) architecture,
is presented in Figure~\ref{fig:1:a}.
The proposed HLN architecture is
presented in Figure~\ref{fig:1:b}.

\begin{figure}
	\centering
	\subfigure[The FCN architecture]{
		\label{fig:1:a}
		\includegraphics[width=2in]{FCN}}
	\hspace{0.2in}
	\subfigure[The HLN architecture]{
		\label{fig:1:b}
		\includegraphics[width=2in]{HLN}}
	\caption{The left is FCN architecture, 
	the right is HLN architecture.
	The big solid circles are the 
	neurons, the medium ones are the 
	middle states of neurons, and 
	the small ones are the vector maps 
	$\mathbf{m}^k$ of SOMs, finally the 
	tiny ones are the function modules.
	The parts omitted refers to repeating
	hidden layers connected in same way.}
	\label{fig:1}
\end{figure}

In HLN architecture,
we propose $h(x)$, 
an unifying function for 
SOM, to convert pattern disimilarity
$\|\mathbf{x}-\mathbf{m}_i\|$ to
a semi-supervised learning factor for
different hidden units. 
The semi-supervised
learning factors act as a 
dynamic neuron activation 
sparsity mask for 
each hybrid learning layer.
It works in the similar manner  
as the Dropout 
technique\cite{srivastava2014dropout}, 
enabling the neural networks 
to improve the model robustness 
and prevent
overfitting by learning 
their submodels for 
each batch. However, HLN differs from 
techniques like Dropout because 
HLN does not generate sparsity with 
randomness. HLN uses the SOM
to unsupervisedly learn 
the \emph{static}
policy of the neuron-activation 
distribution for
each hidden layer, the network 
sparsity generator thereby
will stablize with training 
steps, and the
randomness in sparsity will 
disappear automatically.
We propose $h(x)$ as follows,
\begin{equation}
	\delta_{max} = \max_i\left(
	\|\mathbf{m}_i-\mathbf{x}\|+\epsilon
	\right)
	\label{eq:10}
\end{equation}
\begin{equation}
	\delta_{min} = \min_i\left(
	\|\mathbf{m}_i-\mathbf{x}\|+\epsilon
	\right)
	\label{eq:11}
\end{equation}
where $\delta_{max}$ is the maximum 
disimilarity
between an arbitrary vector 
$\mathbf{m}_i$
and the input pattern vector $\mathbf{x}$,
$\delta_{min}$ is minimum one.
The scale of all disimilarities is
\begin{equation}
	\delta_{scale} = 
	\frac{\mathbf{\delta}_{min}}
	{\mathbf{\delta}_{max}-
	\mathbf{\delta}_{min} + \epsilon}
	\label{eq:12}
\end{equation}
We then unify the disimilarities 
and convert them into sparsity 
mask as
\begin{equation}
	h(x)|_{x=\|\mathbf{m}_i-\mathbf{x}\|}=
	\frac{\delta_{max}\cdot\delta_{scale}}
	{\left\|\mathbf{m}_i-\mathbf{x}\right\|
	+\epsilon}-\delta_{scale}
	\label{eq:13}
\end{equation}
where $\mathbf{m}_i$ and $\mathbf{x}$ 
is the same way defined as in 
equation~(\ref{eq:8}).
$\epsilon$ is set at a small constant
value, such as $\epsilon=10^{-5}$,
to avoid 
\emph{division-by-zero} errors,
which rarely happens. 
Notice that
if any other metric is preferred, 
we can
always replace the pattern 
disimilarity
$\|\mathbf{m}_i-\mathbf{x}\|$ with 
other forms.
$f(s,o)$ is the function to combine the 
sparsity mask $s$ generated by $h(\cdot)$ 
and the fully connected linear summation 
output $o$. In most cases, we use the
simple multiplication as the operator,
\begin{equation}
	f(s,o) = s\cdot o = 
	h\left(
	\left\|
	\mathbf{m}_i-\mathbf{y^{k-1}}
	\right\|
	\right)
	\cdot
	\sum_j
	\left(
	w_j^{k,i}y_j^{k-1}
	\right)
	+b^{k,i},k>1
	\label{eq:14}
\end{equation}
As shown in Figure~\ref{fig:1:b}, 
$f(\cdot) $ is followed 
by an activation function, which could 
be any 
arbitrary nonlinear function that 
takes 
only one dimension inputs and 
outputs a 
single real value, 
such as the sigmoid function, 
$tanh(x)$ and ReLU. 
With the new 
architecture, 
we update the forward computing
rules as 
\begin{equation}
	y_i^k(\mathbf{x})=
	\sigma\left(
	h\left(
	\left\|
	\mathbf{m}_i^k-y^{k-1}(\mathbf{x})
	\right\|
	\right)\cdot
	\sum_j\left(
	w_j^{k,i}y_j^{k-1}(\mathbf{x})
	\right) + b^{k,i}
	\right), k>1
	\label{eq:15}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
and for the first hybrid learning layer,  
\begin{equation}
	y_i^1(\mathbf{x})=
	\sigma\left(
	h\left(
	\left\|
	\mathbf{m}_i^1-\mathbf{x}
	\right\|
	\right)\cdot
	\left(
	\sum_j w_j^{1,i}x_j
	\right) + b^{1,i}
	\right)
	\label{eq:16}
\end{equation}
where $m^k_i$ denotes the $i^{th}$ vector
in the $k^{th}$ SOM of the net(all indexes
start from 1), $\sigma$ is a nonlinear 
activation function.

\section{Training the HLNs}
Let's compare the embedding theories between
HLNs and the ones mentioned in the section of
\emph{Related work and backgrounds}.
The existing semi-supervised algorithms
embed a regularizer into the supervised 
learner, making it impossible to train both of
the supervised learner and the unsupervised
separately. As analyzed before, performances
of such regularizer solutions depends on the
standalone supervised pretraining for which
a profound labeled data is required.
However, in our architecture HLN, we assign
each of the learning methods a completely
separate optimizing object, with no priority
orders restricted.\\
For the supervised learning, we may have such
form of optimizing object as
\begin{equation}
	\arg \min_g \sum_{i=1}^L\ell\left(
	g(\mathbf{x}_i),
	\mathbf{\hat{y}}_i
	\right)
	\label{eq:17}
\end{equation}
where $g(\mathbf{x})$ is the function describing
the mapping from the input 
$\mathbf{x}\in\mathbb{R}^d$ to the output
$\mathbf{y}\in\mathbb{R}^n$, parameterized with
the neural connection weights $\mathbf{W}$ and 
the biases $\mathbf{b}$ for all non-input 
layer neurons in the HLN architecture.
This equation~(\ref{eq:17}) may become the 
following one when Enclidean metric be 
applied for the loss,
\begin{equation}
	\arg \min_{\mathbf{W},\mathbf{b}}
	\frac{1}{2}
	\sum_{i=1}^L
	\left\|
	y^k(
	\mathbf{x}_i)\big|_{\mathbf{W},\mathbf{b}}-
	\mathbf{\hat{y}}_i
	\right\|^2
	\label{eq:18}
\end{equation}
For the unsupervised learning, the optimizing
objects are
\begin{equation}
	\arg \min_{\mathbf{m}^k}
	\sum_i^{L+U}
	\left(
	\min_j\left(
	\left\|
	\mathbf{m}^k_j-\mathbf{x}^k_i
	\right\|
	\right)
	\right),\quad k=1,2,\cdots
	\label{eq:19}
\end{equation}
with the predefined notation:
\begin{equation}
	\mathbf{x}_i^k=
	\left\{
		\begin{aligned}
			\mathbf{x}_i,\quad & k=1\\
			y^{k-1}(\mathbf{x}_i),\quad & k>1
		\end{aligned}
	\right.
	,\quad \mathbf{x}_i\in L+U
	\label{eq:20}
\end{equation}

For a net with $M$ hybrid learning layers,
there should be 1 supervised
and $M$ unsupervised optimizing objects.
As we know a multi-objective problem may not 
have a global solution, however in HLNs,
each object is optimized on its own isolated
parameter space, thus each optimizing object
have its corresponding global optimization 
solution, they together makes the whole one.

Using the well-known Back Propagation(BP)
algorithm, we can obtain parameter updating
rules for training the HLNs. 
We're only focusing on differences compared to
original BP algorithm for the FCN model.
Let's denote the linear summation for $i^{th}$ unit
in $k^{th}$ layer as $o_i^k$, then
linear summations of hybrid learning
layers are (with $\mathbf{y}^0=\mathbf{x}$):
\begin{equation}
	o_i^k =
	h\left(
	\left\|
	\mathbf{m}_i^k-\mathbf{y}^{k-1}
	\right\|
	\right)
	\sum_j 
	W_j^{k,i}
	y_j^{k-1},
	\quad k=1,2,\cdots,M
	\label{eq:23}
\end{equation}
Apply the non-linear activation function
(taking the sigmoid as an instance),
\begin{equation}
	\frac{\partial E}
	{\partial o_i^k}=
	\frac{\partial E}
	{\partial y_i^k}
	\sigma'(o_i^k)=
	\frac{\partial E}
	{\partial y_i^k}
	y_i^k\left(
	1-y_i^k
	\right),
	\quad k=1,2,\cdots,M+1
	\label{eq:24}
\end{equation}
Thus the recursive layer gradient computing
is
\begin{equation}
	\frac{\partial E}
	{\partial y_i^{k-1}}=\sum_j
	\frac{\partial E}
	{\partial o_j^k}
	h\left(
	\left\|
	\mathbf{m}_j^k-
	\mathbf{y}^{k-1}
	\right\|
	\right)
	W_j^{k,i}
	\quad k=1,2,\cdots,M
	\label{eq:25}
\end{equation}
The partial error gradients for the
connection parameters $\mathbf{W}$:
\begin{equation}
	\frac{\partial E}
	{\partial W_j^{k,i}}=\sum_i
	\frac{\partial E}
	{\partial o_i^k}
	h\left(
	\left\|
	\mathbf{m}_i^k-
	\mathbf{y}^{k-1}
	\right\|
	\right)
	y_i^{k-1},
	\quad k=1,2,\cdots,M
	\label{eq:27}
\end{equation}
For the partial error gradients on biases:
\begin{equation}
	\frac{\partial E}
	{\partial b^{k,i}}=
	\frac{\partial E}
	{\partial o_i^k}
	\frac{\partial o_i^k}
	{\partial b^{k,i}}=
	\frac{\partial E}
	{\partial o_i^k}
	\times 1,
	\quad k=1,2,\cdots,M+1
	\label{eq:29}
\end{equation}
Thus we can iterate recursively along 
layers with 
equations~(\ref{eq:23}-\ref{eq:29}) to 
update all parameters of the supervised 
training, and the equations corresponding 
to the unsupervised training refer 
directly to equations~(\ref{eq:8}) and 
(\ref{eq:9}).

\section{Empirical Study}
\subsection{Regression capability
experiment using small synthetic data}
A synthetic dataset is used. It maps
2-dimension vectos into 3 classes, where
$3.5\%$ of the noise is mixed to simulate 
a real sampling dataset.
We then apply the data to the HLN(the 
net with HLN architecture) and the 
FCN(the net without),
and compare the regression capabilities 
of the two. 
The FCN and HLN applied in this 
experiment follow the exact design as 
in Figure~\ref{fig:1} with only a single 
hidden layer.
Figure~\ref{fig:2}-\ref{fig:3} 
present the comparison results.

\begin{figure}[h]
	\centering
	\subfigure[]{
		\includegraphics[width=55mm]{mean_accuracy0}
		\label{fig:2:a}
	}
	\hspace{-8mm}
	\subfigure[]{
		\includegraphics[width=55mm]{mean_accuracy1}
		\label{fig:2:b}
	}
	\hspace{-8mm}
	\subfigure[]{
		\includegraphics[width=55mm]{mean_accuracy2}
		\label{fig:2:c}
	}
	\caption{
		Comparison on mean of 
		accuracy with synthetic data:
		$\mu$ is the mean accuracy for
		multiple replays with the same
		configuration.
		$N_{pre}$ is the unlabeled sample 
		volume for pretraining, $\eta$
		is the initial learning rate,
		$\alpha$ is the decay factor of
		learning rate $\eta(t)$.
		}
	\label{fig:2}
\end{figure}

\begin{figure}[h]
	\centering
	\subfigure[]{
		\includegraphics[width=55mm]{devi_accuracy0}
		\label{fig:3:a}
	}
	\hspace{-8mm}
	\subfigure[]{
		\includegraphics[width=55mm]{devi_accuracy1}
		\label{fig:3:b}
	}
	\hspace{-8mm}
	\subfigure[]{
		\includegraphics[width=55mm]{devi_accuracy2}
		\label{fig:3:c}
	}
	\caption{
		Comparison on deviation of test
		accuracy with synthetic data:
		the deviation takes the form as
		$\delta$ is the deviation of
		multiple replays with the same
		configuration as 
		$\delta=\sum_1^N(\gamma_i-\mu)/N$
		, where $\gamma_i$ is $i^{th}$ 
		replay accuracy.
		}
	\label{fig:3}
\end{figure}

An explanation may be helpful:
we split the data into two partitions, 
one is for the training of two
architectures and the another is for
testing, the train/test ratio is 0.8.
Considering the possible impact from
the parameter initialization, 
we replay the complete workflow
of training and testing with varied
random parameter initialization
for each net with different 
configurations.
From Figure~\ref{fig:2}, easy
to see that networks with our HLN 
architecture win over networks without 
in all conditions concerned.
The overall results we may obtain
from this experiment would be:
\begin{itemize}
	\item The HLN architecture 
		learns much faster than the 
		FCN.
	\item The pretraining
		with unlabeled data does improve 
		the performance of HLN, however 
		the improvement is limited.
	\item A constant learning rate is
		more advisal than a decreasing 
		one with steps.
\end{itemize}

Each of the net training is repeated for 
30 times in our experiments, and within
each training, the net parameters are
reinitialized and relearned from the
very beginning of training. Thus we
can analyze the robustness of 
architectures to the parameter
initialization.
Figure~\ref{fig:3} shows the HLN
stablize quickly under all conditions,
however the FCN requires more training
to get itself stablized. The HLN proves
to be robuster to the neural network
hyperparameter variation of parameter 
initialization.
\subsection{Experiments on MNIST}
Experiments on the well-known MNIST
benchmark demonstrate greater enhencement
over the traditional architecture.
Moreover the advantage of HLN increases 
with higher input dimension
and deeper network. 
To enable the input dimension variation,
a max-pooling is applied before the input
of a neural net. The MNIST dataset
provides 50000 images for training and 
10000 images for testing, with each image
resized as $28\times 28$ of pixels. 
A max-pooling with the kernel of 
$2\times 2$
leads to a input dimension of 196, and
particularly a max-pooling with the
kernel $1\times 1$ will keep the original
input dimension as 784. All networks used
are based on architectures in 
Figure~\ref{fig:1}.

\begin{figure}[h]
	\centering
	\subfigure[]{
		\includegraphics[width=42mm]{mnist_testing_accuracy_1}
		\label{fig:4:a}
	}
	\hspace{-7mm}
	\subfigure[]{
		\includegraphics[width=42mm]{mnist_testing_accuracy_2}
		\label{fig:4:b}
	}
	\hspace{-7mm}
	\subfigure[]{
		\includegraphics[width=42mm]{mnist_testing_accuracy_3}
		\label{fig:4:c}
	}
	\hspace{-7mm}
	\subfigure[]{
		\includegraphics[width=42mm]{mnist_testing_accuracy_4}
		\label{fig:4:d}
	}

	\caption{
		Comparison of testing accuracy on 
		MNIST with 4 different network
		models: 
		$196I$ refers to
		the input layer of dimension 196, 
		$30H$ refers to the hidden layer
		of dimension of 30, and $10F$ is
		the fully connected output layer 
		of dimension 10.
		}
	\label{fig:4}
\end{figure}

\begin{figure}[h]
	\centering
	\subfigure[]{
		\includegraphics[width=42mm]{mnist_training_loss_1}
		\label{fig:5:a}
	}
	\hspace{-7mm}
	\subfigure[]{
		\includegraphics[width=42mm]{mnist_training_loss_2}
		\label{fig:5:b}
	}
	\hspace{-7mm}
	\subfigure[]{
		\includegraphics[width=42mm]{mnist_training_loss_3}
		\label{fig:5:c}
	}
	\hspace{-7mm}
	\subfigure[]{
		\includegraphics[width=42mm]{mnist_training_loss_4}
		\label{fig:5:d}
	}

	\caption{
		Comparison of training loss on 
		MNIST with 4 different network models
		}
	\label{fig:5}
\end{figure}

\begin{figure}[h]
	\centering
	\subfigure[]{
		\includegraphics[width=42mm]{mnist_training_sparsity_1}
		\label{fig:6:a}
	}
	\hspace{-7mm}
	\subfigure[]{
		\includegraphics[width=42mm]{mnist_training_sparsity_2}
		\label{fig:6:b}
	}
	\hspace{-7mm}
	\subfigure[]{
		\includegraphics[width=42mm]{mnist_training_sparsity_3}
		\label{fig:6:c}
	}
	\hspace{-7mm}
	\subfigure[]{
		\includegraphics[width=42mm]{mnist_training_sparsity_4}
		\label{fig:6:d}
	}
	\caption{
		Comparison of neuron activation sparsity 
on MNIST with 4 different network models
		}
	\label{fig:6}
\end{figure}

Seen from Figure~\ref{fig:4:a}, 
where a single hidden layer architecture is used,
the FCN requires about 150-200 epoches to stablize 
at its best test accuracy of 93\%, the HLN however 
only takes 4-5 epoches to achieve the same test 
accuracy or a bit higher. 
It's a $40\times$ speed up 
on the training without a sacrifice of the
model capability. The HLN additionally performs
much higher stablity at all time during the whole
training process. This may be due to the hybrid 
learning method used by the HLN, which enables 
the network to learn the neuron activation 
clustering maps(the SOMs) of every non-output layer
especially the input layer at the same time for
the supervised learning. Each sample for the input
is more profoundly taken in use, making the HLN
a much faster learning.

For Figure~\ref{fig:4:b} with a deeper architecture,
the FCN seems fail to learn at all, however our
HLN remains a fast learning. Though the HLN 
architecture seems \emph{slow down} a bit for
a double hidden layer net compared to the single
one's, it satisifies the emprical knowledge: a deeper
architecture brings a tougher parameter tuning.

For Figure~\ref{fig:4:c}, 
we use a max-pooling with a kernel of $1\times 1$,
which equals to original input as in MNIST.
The input dimension of the net is therefore 4 
times larger than that in the first test, however
the HLN learns as fast as the one of low input
dimension with stablity held the same strong.
While the FCN in this situation fails to learn at
all either.

Let's focus on  
Figure~\ref{fig:4:d}, where the HLN takes about 120
epoches to stalize at the best performance and
the FCN seems never start to learn.
Compared to the third test, the networks have a
deeper architecture, making the model more difficult
to learn as expected.  Compared to the second, 
networks in the fourth experiment
have a larger input dimension, which leads to a
tougher learning too. The two factors putting 
together turns out to be even more harmful to
the training efficiency as we may expect in a
linear manner.

Figure~\ref{fig:5} further confirms the 
\emph{facts} 
drawn from results in Figure~\ref{fig:4}.
The test accuracy is not improved at all
with training steps for deeper architectures,
whereas the training loss is dropping, though,
at a very slow pace. This may conclude to a guess:
with the epoch number increasing to a rather larger
one, the training loss may drop to a level where
a visible learning expressed by test accuracy 
improvement at length begins.

Figure~\ref{fig:6} shows the neuron activation
sparsity for every hidden layer in networks.
Notice that a SOM-embedding output in HLNs
is used as a sparsity mask to control, or more
precisely, to increase the sparsity of every
hidden layer. It's obvious that networks with 
the HLN architecture tend to have a higher 
sparsity than ones without seen from 
Figure~\ref{fig:6}.

\section{Conclusions}
We proposed a hybrid learning architecture,
the HLN, for neural networks to learn from
labeled and unlabeled data at the same time
and to achieve a much faster learning with
fair stablity.
We showed the solution to training networks of
the HLN architecture, and also demonstrated 
the advantages in training speed and robustness
to some hyperparameter variation by experimenting
on synthetic data and the MNIST. In the future,
we will investigate if our architecture works well
on more public benchmark datasets.

\section*{Acknowledgements}
This project was partially supported 
by Grants from Natural Science 
Foundation of China
\#71671178/ \#91546201/ \#61202321, 
and the open project of the Key Lab 
of Big Data Mining and
Knowledge Management. 
It was also supported by Hainan 
Provincial Department of Science 
and Technology under Grant 
No. ZDKJ2016021, and by 
Guangdong Provincial Science and
Technology Project 
20162016B010127004.

%% References
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}         ==>>  [#]
%%   \cite[chap. 2]{key} ==>> [#, chap. 2]
%%


%% References with BibTeX database:

\bibliographystyle{elsarticle-num}
\bibliography{ref}

\end{document}

%%
%% End of file `procs-template.tex'.
